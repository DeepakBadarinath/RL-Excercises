{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robot Learning\n",
    "\n",
    "## Assignment 3\n",
    "\n",
    "#### Group names: Edit this cell and write your names here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Consider the following $10 \\times 10$ grid world:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"helpers/gridworld_sketch.png\" alt=\"Grid World\" title=\"Grid World\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent may start in any cell that is not an obstacle nor the goal.\n",
    "\n",
    "It can choose between eight actions, which correspond to moving to the directions \n",
    "\n",
    "$$a_i \\in \\{NW,      N,      NE,     E,     SE,    S,     SW,     W\\}$$\n",
    "\n",
    "These are indexed according to the order above, i.e. $a_0 = NW$ and $a_6 = SW$.\n",
    "\n",
    "The agent must be careful, for the actions are non-deterministic! The agent moves with probability $0.9$ into the desired\n",
    "direction, but with probability $0.05$ deviates $45^{\\circ}$ to the left and with probability $0.05$ deviates $45^{\\circ}$ \n",
    "to the right of the desired direction due to treacherous gusts unexpectedly sweeping the grid.\n",
    "\n",
    "The rewards are structured as follows:\n",
    "\n",
    "* When it reaches a blue cell, it receives a little snack of 10 points.\n",
    "\n",
    "* All actions entering a white cell receive -1 point.\n",
    "\n",
    "* When the agent reaches the green goal cell, it receives 100 points and the episode ends.\n",
    "\n",
    "* When it attempts to enter a red obstacle cell, it receives -20 points and stays in the cell it came from. This does not additionally yield the reward of the cell it came from.\n",
    "\n",
    "* When it attempts to leave the grid, it receives -30 points and stays in the cell it came from. This does not additionally yield the reward of the cell it came from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 11\n",
    "\n",
    "To familiarize yourself with the environment above, answer the following questions:\n",
    "\n",
    "<div style=\"text-align: right; font-weight:bold\"> 2 + 1 + 2 = 5 Points </div>\n",
    "\n",
    "* The agent is at $s = (y_s, x_s) = (3, 6)$ and wants to execute $a_5$. What is the probability $P^{a_5}_{s,s'}$ for $s' =(4,7)$ of moving from state $s$ to follow-up state $s'$ when executing the action $a_5$?\n",
    "\n",
    "* The agent is at $s = (5, 4)$ and wants to execute $a_7$. What is the probability $P^{a_7}_{s,s'}$ for $s'=s$?\n",
    "\n",
    "* The agent is at $s = (3, 8)$ and wants to execute $a_3$. What is the expected value of the reward?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1)If the agent is on  $s = (y_s, x_s) = (3, 6)$ and wants to execute $a_5$(S), the probability to move from state $s$ to follow-up state $s'$ is:\n",
    "\n",
    "$P^{a_5}_{s,s'}$= $0.05\\times0.05\\times0.05\\times0.05\\times0.9\\times0.9\\times0.05\\times0.05\\times0.05\\times0.05\\times0.05$=$1.58\\times 10^{-12}$\n",
    "\n",
    "The agent goes 4 times to W with the probability 0.05. Then it goes 2 times to S with the probability of 0.9 and then 5 more times to E with the probability of 0.05.\n",
    "\n",
    "2)If the agent is on  $s = (y_s, x_s) = (5, 4)$ and wants to execute $a_7$ (W), the probaility for $s'$=$s$ is:\n",
    "$P^{a_7}_{s,s'}$= $Pr\\{s_{t+1}=s'|s_{t}=s, a_{t}=a_{7}\\}$ + $Pr\\{s_{t+1}=s'|s_{t}=s, a_{t}=a_{7}\\}$ = 0.05 + 0.05\n",
    "=0.10 \\\n",
    "To stay on the same cell, the agent must move towards the upper obstacle (4, 4) or the lower one (6, 4).\n",
    "Since the agent goes to W with the probability 0.9, the probability is 0.05 according to N (upper obstacle) and 0.05 according to S(lower obstacle). Both probabilitys have to be summed up.\n",
    "\n",
    "3)If the agent is on  $s = (y_s, x_s) = (3, 8)$ and wants to execute $a_3$, the expected value of the reward is:\n",
    "\n",
    "$R^{a_3}_{s,s'}$=$0.9\\times100+0.05\\times(-20)+0.05\\times(-1)$=90.05\n",
    "If the agent moves in direction E he will reach the green goal with probability 0.9 (100 points), with probability 0.05 he will reach a obstacle (-20 points) and with probability 0.05 a white cell (-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 12\n",
    "\n",
    "Using the *Iterative Policy Evaluation* Algorithm, compute the value $V^{\\pi}(s)$ of all accessible cells $s$ for a policy $\\pi(s,a)$ that chooses with probability $0.5$ a random action and otherwise moves to the right.\n",
    "\n",
    "Intialize $V(s)$ with zero, use a discount parameter of $\\gamma=0.9$ and show your results by printing your state values $V^{\\pi}(s)$.\n",
    "\n",
    "<div style=\"text-align: right; font-weight:bold\"> 5 Points </div>\n",
    "\n",
    "#### Note\n",
    "\n",
    "For your convenience, you are provided the helper function *getNextStatesRewardsAndProbabilities(state, action)* which returns for a given state $s$ and an action $a$ a list of 3 -tuples of the form\n",
    "\n",
    "$$[(s_0', R^a_{s,s_0'}, P^a_{s,s_0'}), (s_1', R^a_{s,s_1'}, P^a_{s,s_1'}), \\dots]$$\n",
    "\n",
    "where $s_i'$ are all future states with $P^a_{s,s_i'} \\neq 0$. Here $s = (y, x)$ and $s_i' = (y_i', x_i')$ are both tuples of integers, $a \\in {0, \\dots, 7}$ is an integer, and $R^a_{s,s_i'}$, $P^a_{s,s_i'}$ are both floats.\n",
    "\n",
    "Also, please find below some data structures which you might find helpful and create code and text cells as necessary to present your solution!\n",
    "\n",
    "In your implementation, $V(s)$ should be a $10 \\times 10$ numpy array and $\\pi(s,a)$ should be a $10 \\times 10 \\times 8$ numpy array, where $\\sum_a \\pi(s,a) = 1$ for all s!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sPrime: (0, 6) R: -30.0 P: 0.05\n",
      "sPrime: (0, 7) R: 10.0 P: 0.9\n",
      "sPrime: (1, 7) R: -1.0 P: 0.05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from helpers.utils import getNextStatesRewardsAndProbabilities\n",
    "\n",
    "# this is a list of all states\n",
    "states = [(y,x) for y in range(10) for x in range(10)]\n",
    "# this is a list of all states containing obstacles\n",
    "obstacles = [(1,6), (1,8), (2,1), (2,2), (2,3), (2,4), (2,5), (2,6), (2,8), \\\n",
    "             (3,1), (3,7), (4,3), (4,4), (4,5), (4,6), (5,8), \\\n",
    "             (6,1), (6,2), (6,3), (6,4), (6,5), (6,6), (6,7),\\\n",
    "             (8,8), (8,9), (9,4), (9,8), (9,9)]\n",
    "# this is a list containing all goal states\n",
    "terminalStates = [(3,9)]\n",
    "#this is an array containing all actions\n",
    "actions = np.array([0, 1, 2, 3, 4, 5, 6, 7]) #[NW,      N,      NE,     E,     SE,    S,     SW,     W]\n",
    "# example of how to unpack getNextStatesRewardsAndProbabilities(state, action):\n",
    "# create dummy state and action\n",
    "s_test = (0,6)\n",
    "a_test = 3\n",
    "# how to call helper function and loop over the return values\n",
    "for sPrime, R, P in getNextStatesRewardsAndProbabilities(state=s_test, action=a_test):\n",
    "    print('sPrime:', sPrime, 'R:', R, 'P:', P)\n",
    "    \n",
    "# once you have state values V, you can print them with okay'ish formatting like so:\n",
    "#print(\"State Values:\")\n",
    "#print(np.around(V, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 13\n",
    "\n",
    "Now it is time to find a good policy. Use the *Policy Iteration* algorithm to compute the optimal value $V^*(s)$ for each accessible cell. Please make sure to apply *Policy Iteration* exhaustively, which means to let policy evaluation converge every time before applying policy improvement.\n",
    "\n",
    "Retrieve the resulting optimal-policy $\\pi^*(s)$. To obtain a greedy policy given $V(s)$, make use of:\n",
    "\n",
    "$$\\pi_{greedy}(s) := \\operatorname{argmax}_a Q(s,a) = \\operatorname{argmax}_a \\sum_{s'}P_{ss'}^a\\cdot[R_{ss'}^a+\\gamma\\cdot V(s')]$$\n",
    "\n",
    "As implied by these terms, we recommend using intermediate $Q$-values, shaped $10 \\times 10 \\times 8$ for this step!\n",
    "\n",
    "Finally, present your results by printing $V^*(s)$ and using our helper function *drawPolicy()* to visualize $\\pi^*(s,a)$.\n",
    "\n",
    "<div style=\"text-align: right; font-weight:bold\"> 5 Points </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.utils import drawPolicy\n",
    "# print state values with\n",
    "#print(\"State Values:\")\n",
    "#print(np.around(V, 1))\n",
    "# then show policy using helper function as below\n",
    "# usage of the helper function, where pi is a (10,10,8) numpy array representing a deterministic policy:\n",
    "# make sure that all entries for pi(s,a) are 0 for all but one action a*, for which pi(s,a*) = 1.0\n",
    "#drawPolicy(pi)\n",
    "# this will plot arrows representing your policies into the grid world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 14\n",
    "\n",
    "Use the *Value Iteration* algorithm to compute the optimal value $V^*(s)$ for each cell. Make sure to reinitialize $V(s)$ with zero. You can sanity-check your results by comparing with $V^*(s)$ from the previous task.\n",
    "\n",
    "Finally, present your results by printing $V^*(s)$ and using our helper function *drawPolicy()* to visualize $\\pi^*(s,a)$.\n",
    "\n",
    "<div style=\"text-align: right; font-weight:bold\"> 5 Points </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print state values with\n",
    "#print(\"State Values:\")\n",
    "#print(np.around(V, 1))\n",
    "# then show policy using helper function as below\n",
    "# usage of the helper function, where pi is a (10,10,8) numpy array representing a deterministic policy:\n",
    "# make sure that all entries for pi(s,a) are 0 for all but one action a*, for which pi(s,a*) = 1.0\n",
    "#drawPolicy(pi)\n",
    "# this will plot arrows representing your policies into the grid world."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
