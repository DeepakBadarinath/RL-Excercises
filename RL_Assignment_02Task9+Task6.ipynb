{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robot Learning\n",
    "\n",
    "## Assignment 2\n",
    "\n",
    "#### Group names: Edit this cell and write your names here\n",
    "\n",
    "### Introduction\n",
    "\n",
    "\n",
    "Please read the theoretical questions and practical tasks below, and answer them in a text or code cell as suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "List the components of a *Markov Decision Process* (MDP) and give a short explanation for each.\n",
    "\n",
    "<div style=\"text-align: right; font-weight:bold\"> 4 Points </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "A Markov Decision Process (MDP) consists of sets for State and Action (S, A), transition probabilitoes and expected reward.\n",
    "\n",
    "Sets for State and Action (S, A):  \\\n",
    "-The state (S) means all available information from the agent about the environment \\\n",
    "-A stands for the action the agent is doing.\n",
    "\n",
    "Transition probabilities: \\\n",
    "$P_{ss'}$ = $Pr\\{ s_{t+1}=s'|s_{t}=s,a_{t}=a\\} for \\, all \\,  s,s'$ $\\in$S,a $\\in$ A(s) \\\n",
    "Transition Probability is the probability that the agend move from one state to another. The formula means the transition from s to s'. So that in the next transition is s'under the condition that one starts at $s_{t}$ and the action a has been done.\n",
    "\n",
    "Expected reward : \\\n",
    "$R_{ss'}$ = $E\\{ r_{t+1} | s_{t}=s,a_{t}=a,s_{t+q}=s'\\} for \\, all\\,  s,s'$ $\\in$S,a $\\in$ A(s)\n",
    "\n",
    "Expacted rewards is the expected value that the agent receives when the action has been taken is in state S in the environemt. The formula means, the reward we get under the condition that we start at $s_{t}$ and our new situation is s'.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "\n",
    "Recall the state transition diagram for the Recycling Robot discussed in the lecture:\n",
    "\n",
    "<img src=\"state_diagram_new.png\" alt=\"Transition Table\" title=\"Transition Table for the recycling robot\" width=\"650\"/>\n",
    "    \n",
    "   \n",
    "In the markdown cell below, fill in the missing values for $P_{ss'}^{a}$ and $R_{ss'}^{a}$ in the table.\n",
    "    \n",
    "\n",
    "<div style=\"text-align: right; font-weight:bold\"> 3 Points </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "Your answer to the above question goes here.\n",
    "\n",
    "| Current State | Next State | Action | Transition Probability | Expected Reward |\n",
    "|:-------------:|:-----------:|:---------:|:-----------------------:|:----------------:|\n",
    "| $s$           | $s'$        | $a$       | $P_{ss'}^{a}$           | $R_{ss'}^{a}$    |\n",
    "| *low*         | *low*       | *search*  |           ?             |        ?         |\n",
    "| *low*         | *high*      | *wait*    |           ?             |        ?         |\n",
    "| *low*         | *low*       | *recharge*|           ?             |        ?         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "\n",
    "Derive the Bellman Equation for $V^{\\pi}$:\n",
    "\n",
    "$$V^{\\pi}(s) = \\sum_a \\pi(s,a) \\sum_{s'} P_{ss'}^{a} [R_{ss'}^{a} + \\gamma V^{\\pi}(s')]$$\n",
    "\n",
    "You can start at the definition\n",
    "\n",
    "$$V^{\\pi}(s) = \\mathbb{E}_{\\pi}[R_t | s_t = s]$$\n",
    "\n",
    "<div style=\"text-align: right; font-weight:bold\"> 4 Points </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "Your answer to the above question goes here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "\n",
    "Softmax action selection and $\\varepsilon$-greedy action selection are two strategies to encourage exploration in the action selection process of an agent.\n",
    "\n",
    "Which of those two would you prefer for implementing the policy of an agent?\n",
    "Give at least two arguments for your choice.\n",
    "\n",
    "<div style=\"text-align: right; font-weight:bold\"> 4 Points </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "We would use the Softmax action selection to implement the agent's policy.\n",
    "\n",
    "The Softmax Action Selection has the advantage that the $\\tau$ (control parameter) is automatically selected depending on the temperature. With $\\varepsilon$-greedy action selection, $\\varepsilon$ must be selected initially and is not automatically.\n",
    "\n",
    "Another advantage of the Softmax action selection is that when $\\tau$ approaching zero Softmax action selection also becomes a greed action selection. If $\\tau$ doesn't get close to zero, there isn't just one greedy action selection.\n",
    "\n",
    "A disadvantage of $\\varepsilon$-greedy is that there is random action with probability $\\varepsilon$ there. If it chooses the same among all, it is also possible that the best appearing choice is not made. A greedy choice does not necessarily mean that the best choice has been made.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10\n",
    "\n",
    "Regard the following function, which implements a low-pass filter:\n",
    "\n",
    "$$z(k+1) = z(k)+\\alpha[x-z(k)]$$\n",
    "\n",
    "By completing the code cell below, depict the development of $z(k)$ when $z(0) = 0.1$ and $x = 1.0$ and $\\alpha = 0.05$ for $500$ time steps $k$.                 \n",
    "\n",
    "Also by completing the code cell below, implement the case that $x$ is time dependent; specifically that $x(k)$ is a periodic function with period 200 defined by\n",
    "\n",
    "$$x(k)= \n",
    "\\begin{cases}\n",
    "    0,& \\text{if } k < 100\\\\\n",
    "    1,              & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "If you would like to use your own code instead of the templates below, feel free to do so instead, adding or removing cells as necessary!\n",
    "\n",
    "<div style=\"text-align: right; font-weight:bold\"> 5 Points </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Answer\n",
    "#Your answer to the above question goes here.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#inputs: z - list, list of values on the trajectory of z so far\n",
    "#        x - float, the most recently observed value x\n",
    "#        alpha = float, filter constant\n",
    "#output: z[k+1] - float, the next value on the trajectory of z\n",
    "def step(z, x, alpha):\n",
    "    #TODO: calculate and return z[k+1], you can use z[-1] to get z[k]\n",
    "\n",
    "#inputs: k - np.array, a numpy array of N equidistant time steps, shape (N,)\n",
    "#        constant - Bool, whether to return constant values x=C or non-trivial function values x(k)\n",
    "#output: x(k) - np.array, a numpy array with shape (N,). Make sure this has the same shape as k even in constant mode!\n",
    "def x(k, constant):\n",
    "    #constant x\n",
    "    if constant:\n",
    "        #TODO: return constant values x of correct shape\n",
    "    #time dependent x(k)\n",
    "    else:\n",
    "        #TODO: return x(k) as described in the task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cell below (without modifying it) to present your results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You do not need to modify this cell.\n",
    "#If your code above is fine you will get a visualization of your results from this cell.\n",
    "\n",
    "#starting value for z\n",
    "z_trajectory = [0.1]\n",
    "#time-steps\n",
    "k = np.arange(500)\n",
    "#values x(k) for constant x\n",
    "x_constant_trajectory = x(k=k, constant=True)\n",
    "#simulate filter output for constant input signal\n",
    "for x_value in x_constant_trajectory:\n",
    "    z_next = step(z=z_trajectory, x=x_value, alpha=0.05)\n",
    "    z_trajectory.append(z_next)\n",
    "    \n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(9,3))\n",
    "ax1.plot(x_constant_trajectory, c='r')\n",
    "ax1.set_title('Input Signal')\n",
    "ax1.set_xlabel('time step k')\n",
    "ax1.set_ylabel('signal value')\n",
    "ax2.plot(z_trajectory, c='b')\n",
    "ax2.set_title('Output Signal')\n",
    "ax2.set_xlabel('time step k')\n",
    "ax2.set_ylabel('filter value')\n",
    "fig.suptitle('Constant Input')\n",
    "plt.show()\n",
    "\n",
    "#time dependent system input\n",
    "z_trajectory = [0.1]\n",
    "x_trajectory = x(k=k, constant=False)\n",
    "for x_value in x_trajectory:\n",
    "    z_next = step(z=z_trajectory, x=x_value, alpha=0.05)\n",
    "    z_trajectory.append(z_next)\n",
    "    \n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(9,3))\n",
    "ax1.plot(x_trajectory, c='r')\n",
    "ax1.set_title('Input Signal')\n",
    "ax1.set_xlabel('time step k')\n",
    "ax1.set_ylabel('signal value')\n",
    "ax2.plot(z_trajectory, c='b')\n",
    "ax2.set_title('Output Signal')\n",
    "ax2.set_xlabel('time step k')\n",
    "ax2.set_ylabel('filter value')\n",
    "fig.suptitle('Time Dependent Input')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
