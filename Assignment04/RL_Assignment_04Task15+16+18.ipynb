{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robot Learning\n",
    "\n",
    "## Assignment 4\n",
    "\n",
    "#### Group names: Edit this cell and write your names here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 15\n",
    "\n",
    "Recall the proof of the Policy Improvement Theorem presented in the lecture. In this context, briefly explain the equality:\n",
    "\n",
    "$$E_{\\pi'}\\{r_{t+1}+ \\gamma Q^{\\pi}(s_{t+1}, \\pi'(s_{t+1}))|s_t = s\\}=E_{\\pi'}\\{r_{t+1}+ \\gamma E_{\\pi'}\\{r_{t+2}+\\gamma V^{\\pi}(s_{t+2})\\}|s_t = s\\}$$\n",
    "\n",
    "<div style=\"text-align: right; font-weight:bold\"> 2 Points </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we assume that the action value function, in state s, take something different that our policy tells us, is larger than the expactet value follwing the policy $\\pi$.\n",
    "\n",
    "$$V^{\\pi}(s) \\leq  Q^{\\pi}(s, \\pi'(s))$$\n",
    "\n",
    "\n",
    "This is our expected return, assuming the $\\pi'$ was chosen, under the situation s.\n",
    " $$=E_{\\pi'}\\{r_{t+1}+ \\gamma V^{\\pi}(s_{t+1})|s_t = s\\}$$\n",
    "\n",
    "\n",
    "Which is the expected return in the following up situation because we assume $V^{\\pi}(s) \\leq  Q^{\\pi}(s, \\pi'(s))$, \n",
    "We can replace $V^{\\pi}(s_{t+1})$ with $Q^{\\pi}(s_{t+1}, \\pi'(s_{t+1}))$, \n",
    "\n",
    "$$\\leq E_{\\pi'}\\{r_{t+1}+ \\gamma Q^{\\pi}(s_{t+1}, \\pi'(s_{t+1}))|s_t = s\\}$$\n",
    "\n",
    "\n",
    "Now we're just simplifying because we know from the lecture that $Q^{\\pi}(s,a)$ $=E_{\\pi'}\\{r_{t+1} +\\gamma V^{\\pi}s_{t+1})\\|s_{t}=s,a_{t}=a\\}$\n",
    "So the respective action value function $Q^{\\pi}(s,a)$ tells us how valuable it is to take the action a in state s here. And the action value function is the respective return, which is the reward we got directly and the reward of all the forecomming steps. All the forecomming steps can be sum up in the state Value of the following up states.\n",
    "\n",
    "$$=E_{\\pi'}\\{r_{t+1}+ \\gamma E_{\\pi'}\\{r_{t+2}+\\gamma V^{\\pi}(s_{t+2})\\}|s_t = s\\}$$\n",
    "\n",
    "Now we take some rewards of the expactation, we accumulate our forecoming rewards\n",
    "\n",
    "$$=E_{\\pi'}\\{r_{t+1}+ \\gamma r_{t+2}+\\gamma^2 V^{\\pi}(s_{t+2})\\|s_t = s\\}$$\n",
    "\n",
    "\n",
    "Now we take it out\n",
    "\n",
    "$$\\leq E_{\\pi'}\\{r_{t+1}+ \\gamma r_{t+2}, \\gamma^2 r_{t+3}, \\gamma^3 V^{\\pi}(s_{t+3}|s_t = s\\}$$\n",
    "\n",
    "\n",
    "\n",
    "$$...$$\n",
    "\n",
    "If we continue this steps to the infinity, we end up with the state value function being in the Situation s of the new policy $\\pi'$\n",
    "\n",
    "$$\\leq E_{\\pi'}\\{r_{t+1}+ \\gamma r_{t+2}, \\gamma^2 r_{t+3}, \\gamma^3 r_{t+4} + .....|s_t = s\\}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$=V^{\\pi'}(s)$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 16\n",
    "\n",
    "Compare Monte Carlo and Temporal Difference policy evaluation. Give at least two advantages of one over the other.\n",
    "\n",
    "<div style=\"text-align: right; font-weight:bold\"> 4 Points </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Temporal Difference policy evaluation**\n",
    "\n",
    "$*$Monte Carlo policy evaluations can suffer from a high viariance. One advantage of Temporal difference poilcy is that there is not this kind of problem.\n",
    "\n",
    "$*$Temporal Difference policy evaluation exploits the markov property, what Monte Carlo policy evaluation does not.\n",
    "\n",
    "\n",
    "$*$In practice, Temporal Difference policy evaluation is faster\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Monte Carlo policy evaluation**\n",
    "\n",
    "$*$ Monte Carlo policy evaluation is unbiased. Because of the initial states, Temporal Difference policy evaluation is biased.   \n",
    "\n",
    "\n",
    "$*$ Monte Carlo policy evaluation is not very sensitive to initial value, Temporal Difference policy evaluation is sensitive to the initial value.\n",
    "\n",
    "\n",
    "$*$ Temporal Difference policy evaluation learn by bootstrapping. Monte Carlo policy evaluation does not bootstrap\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the table below, indicate whether the attribute in the column is descriptive of the algorithm in the row.\n",
    "\n",
    "|                                       | Evaluation Method | Control Method |    Samples | Bootstraps |\n",
    "|:-------------------------------------:|:-----------------:|:--------------:|:----------:|:----------:|\n",
    "|Iterative Policy Evaluation            |$\\checkmark$       |$\\times$        |$\\times$    |$\\checkmark$|\n",
    "|Monte Carlo Policy Evaluation          |?                  |?               |?           |?           |\n",
    "|Temporal Difference Policy Evaluation  |?                  |?               |?           |?           |\n",
    "|Value Iteration                        |?                  |?               |?           |?           |\n",
    "|Policy Iteration                       |?                  |?               |?           |?           |\n",
    "|SARSA                                  |?                  |?               |?           |?           |\n",
    "|Q-Learning                             |?                  |?               |?           |?           |\n",
    "\n",
    "<div style=\"text-align: right; font-weight:bold\"> 6 Points </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 18\n",
    "\n",
    "Consider the following $18 \\times 14$ grid world, which represents a section of a racetrack with a sharp right turn after a lane narrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"helpers/racetrack.png\" alt=\"Grid World\" title=\"Grid World\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent enters the section on one of the blue cells at the bottom. Its initial velocity is one cell per time step into the upward direction.\n",
    "\n",
    "Actions are to increase or decrease its velocity components by one or to leave it unchanged. To clarify, the actions are not to set a velocity, but to accelerate or decelerate. The velocity is thus part of the agent's state, the available actions are accelerations within the physical limitations of the vehicle that the agent is driving.\n",
    "\n",
    "Both velocity components are restricted to be nonnegative and their sum must be at least 1 and cannot exceed 5. This constrains the available actions based on the agent's current velocity.\n",
    "\n",
    "The rewards are −1 for each step that the agent stays on the track, which is colored green. The reward is −10 if the agent drives into the crash barriers marked in red. \n",
    "\n",
    "The episode ends when the agent crashes or when it succesfully exits the section by driving onto the blue line at the top-right of the grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given some information for this environment. Please find it in the data structures below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "racetrack = -10. * np.ones((18,14))\n",
    "\n",
    "circuit =   [(1,x) for x in np.arange(5,13)] + \\\n",
    "            [(2,x) for x in np.arange(4,13)] + \\\n",
    "            [(3,x) for x in np.arange(3,13)] + \\\n",
    "            [(4,x) for x in np.arange(3,13)] + \\\n",
    "            [(5,x) for x in np.arange(3,13)] + \\\n",
    "            [(6,x) for x in np.arange(3,11)] + \\\n",
    "            [(7,x) for x in np.arange(3,9)] + \\\n",
    "            [(8,x) for x in np.arange(3,8)] + \\\n",
    "            [(9,x) for x in np.arange(3,8)] + \\\n",
    "            [(10,x) for x in np.arange(3,8)] + \\\n",
    "            [(11,x) for x in np.arange(4,8)] + \\\n",
    "            [(12,x) for x in np.arange(4,8)] + \\\n",
    "            [(13,x) for x in np.arange(4,7)] + \\\n",
    "            [(14,x) for x in np.arange(4,7)] + \\\n",
    "            [(15,x) for x in np.arange(2,7)] + \\\n",
    "            [(16,x) for x in np.arange(2,7)]\n",
    "            \n",
    "\n",
    "for cell in circuit:\n",
    "    racetrack[cell] = -1.\n",
    "\n",
    "finishLine = [(y,13) for y in np.arange(1,6)]\n",
    "\n",
    "for cell in finishLine:\n",
    "    racetrack[cell] = 0.\n",
    "\n",
    "startingGrid = [(17,x) for x in np.arange(2,7)]\n",
    "\n",
    "for cell in startingGrid:\n",
    "    racetrack[cell] = -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, please familiarize yourself with the provided racetrack environment class and helper functions by examining the example episode below, which is played by an agent that makes random decisions at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-10.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x1296 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the required helper class and visualization function\n",
    "from utils import env, drawTrajectory\n",
    "\n",
    "# this makes sure that our description of the episode visualization makes sense\n",
    "np.random.seed(321)\n",
    "\n",
    "\n",
    "# create a racing environment\n",
    "racer = env()\n",
    "# reset the agent to one of the 4 starting cells at random\n",
    "# do this before each new episode\n",
    "racer.reset()\n",
    "# you can also manually set the agent position if you want to examine its behaviour there\n",
    "#racer.setPosition(17,6)\n",
    "# create flag tracking whether the episode is over\n",
    "done = False\n",
    "\n",
    "# create lists to save the trajectory and the decisions of the agent\n",
    "actions = []\n",
    "moves = []\n",
    "velocities = []\n",
    "\n",
    "# these methods do what you would expect\n",
    "# v is an integer in [0, 35] representing a velocity\n",
    "# y, x are both integers, together, they represent the agents location\n",
    "v = racer.getVelocity()\n",
    "y, x = racer.getPosition()\n",
    "\n",
    "# simulate until the episode is over\n",
    "while not done:\n",
    "    # which actions are possible at the current velocity?\n",
    "    # the actions are integers in [0,8]\n",
    "    possibleActions = racer.getAvailableActions()\n",
    "    # select one of them randomly\n",
    "    a = np.random.choice(possibleActions)\n",
    "    # save the action\n",
    "    actions.append(a)\n",
    "    # execute the action using the step function\n",
    "    # y, x, v are new positions and velocities\n",
    "    # done is the flag whether the episode is over\n",
    "    # travel details exactly which cells the agent traveled through\n",
    "    y, x, v, r, done, travel = racer.step(action = a)\n",
    "    print(r)\n",
    "    # save the velocities\n",
    "    velocities.append(v)\n",
    "    # save the cells that the agent most recently passed through\n",
    "    # they are only needed for visualization\n",
    "    moves.append(travel)\n",
    "\n",
    "# use the helper function to display how the episode went\n",
    "drawTrajectory(moves, actions, velocities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an output to the cell above, you get a visualization of the agent's path through the race track section. You can see several cells marked in dark blue. These are the grid cells that the agent passed through. Between two time steps, they are calculated by a line drawing algorithm given the current position and velocity of the agent, as well as the acceleration it selected. This acceleration is represented by the arrows with the blue tips. In some cases, a cell will instead contain a circle. This represents the action of no acceleration or deceleration at all, which means that the velocity stays the same as in the previous time step. The red arrows show the velocity of the agent at a certain step after the acceleration the agent selected is added to it. For example, at the very start, the agent has the initial velocity of one cell per step towards the top of the grid. However, it chooses to decelerate in y direction and accelerate towards the right. Thus, the resulting velocity, represented by the first red arrow, points towards the right. Unfortunately, the agent in this episode did not slow down on time and the episode ends in a crash into the barriers at the top of the grid. Note that the visualization is not central to this task, as you are not asked to compute an optimal policy for this environment (yet). However, we feel that it might help you get some intuition for the challenges the agent faces. These should be, to a certain extent, reflected in the state values $V(s)$ that you get.\n",
    "\n",
    "#### Now to the task: \n",
    "\n",
    "Compute  $V(s)$  for each grid cell on the racetrack (obstacle cells do not matter) using either constant-$\\alpha$ Temporal Difference *or* constant-$\\alpha$ every-visit Monte Carlo (your choice!) Policy Evaluation for  10000  episodes of a policy that uniformly chooses one of the available actions. Use $\\alpha=0.2$ and if applicable, $\\gamma=0.9$. Print and visualize the resulting state values!\n",
    "\n",
    "<div style=\"text-align: right; font-weight:bold\"> 8 Points </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution goes here.\n",
    "#Constant alpha TD \n",
    "\n",
    "from utils import env, drawTrajectory\n",
    "\n",
    "# this makes sure that our description of the episode visualization makes sense\n",
    "#np.random.seed(321)\n",
    "noEpisodes = 10000\n",
    "alpha = 0.2\n",
    "gamma = 0.9\n",
    "\n",
    "V = np.zeros(shape = (18,14,36))\n",
    "#V[xcoordinate,ycooordinate,velocity]\n",
    "\n",
    "\n",
    "for episode in range(noEpisodes):\n",
    "    \n",
    "    # create a racing environment\n",
    "    racer = env()\n",
    "    # reset the agent to one of the 4 starting cells at random\n",
    "    # do this before each new episode\n",
    "    racer.reset()\n",
    "    # you can also manually set the agent position if you want to examine its behaviour there\n",
    "    #racer.setPosition(17,6)\n",
    "    # create flag tracking whether the episode is over\n",
    "    done = False\n",
    "\n",
    "    # create lists to save the trajectory and the decisions of the agent\n",
    "    actions = []\n",
    "    moves = []\n",
    "    velocities = []\n",
    "\n",
    "    # these methods do what you would expect\n",
    "    # v is an integer in [0, 35] representing a velocity\n",
    "    # y, x are both integers, together, they represent the agents location\n",
    "    v = racer.getVelocity()\n",
    "    y, x = racer.getPosition()\n",
    "\n",
    "    # simulate until the episode is over\n",
    "    while not done:\n",
    "        # which actions are possible at the current velocity?\n",
    "        # the actions are integers in [0,8]\n",
    "        possibleActions = racer.getAvailableActions()\n",
    "        # select one of them randomly\n",
    "        a = np.random.choice(possibleActions)\n",
    "        # save the action\n",
    "        actions.append(a)\n",
    "        # execute the action using the step function\n",
    "        # y, x, v are new positions and velocities\n",
    "        # done is the flag whether the episode is over\n",
    "        # travel details exactly which cells the agent traveled through\n",
    "        \n",
    "        y_prime, x_prime, v_prime, r, done, travel = racer.step(action = a)\n",
    "        \n",
    "        V[y,x,v] += alpha * (r + gamma*V[y_prime,x_prime,v_prime] - V[y,x,v])\n",
    "        \n",
    "        y, x, v = y_prime, x_prime, v_prime\n",
    "        \n",
    "        # save the velocities\n",
    "        velocities.append(v)\n",
    "        # save the cells that the agent most recently passed through\n",
    "        # they are only needed for visualization\n",
    "        moves.append(travel)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        ...,\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ]],\n",
       "\n",
       "       [[ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        ...,\n",
       "        [ 0.     , -3.728  , -0.7184 , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     , -4.304  ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ]],\n",
       "\n",
       "       [[ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        ...,\n",
       "        [ 0.     , -0.54272,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        ...,\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ]],\n",
       "\n",
       "       [[ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        ...,\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ]],\n",
       "\n",
       "       [[ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        ...,\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,\n",
       "          0.     ]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constant alpha EveryVisit MonteCarlo\n",
    "from utils import env, drawTrajectory\n",
    "\n",
    "# this makes sure that our description of the episode visualization makes sense\n",
    "#np.random.seed(321)\n",
    "noEpisodes = 10000\n",
    "alpha = 0.2\n",
    "gamma = 0.9\n",
    "\n",
    "V = np.zeros(shape = (18,14,36))\n",
    "#V[xcoordinate,ycooordinate,velocity]\n",
    "\n",
    "for episode in range(noEpisodes):\n",
    "    \n",
    "    # create a racing environment\n",
    "    racer = env()\n",
    "    # reset the agent to one of the 4 starting cells at random\n",
    "    # do this before each new episode\n",
    "    racer.reset()\n",
    "    # you can also manually set the agent position if you want to examine its behaviour there\n",
    "    #racer.setPosition(17,6)\n",
    "    # create flag tracking whether the episode is over\n",
    "    done = False\n",
    "\n",
    "    # create lists to save the trajectory and the decisions of the agent\n",
    "    actions = []\n",
    "    moves = []\n",
    "    velocities = []\n",
    "    \n",
    "    #create list to save all possible states traversed by agent\n",
    "    states = []\n",
    "    rewards = []\n",
    "\n",
    "    # these methods do what you would expect\n",
    "    # v is an integer in [0, 35] representing a velocity\n",
    "    # y, x are both integers, together, they represent the agents location\n",
    "    v = racer.getVelocity()\n",
    "    y, x = racer.getPosition()\n",
    "\n",
    "    # simulate until the episode is over\n",
    "    while not done:\n",
    "        \n",
    "        states.append((y,x,v))\n",
    "        \n",
    "        # which actions are possible at the current velocity?\n",
    "        # the actions are integers in [0,8]\n",
    "        possibleActions = racer.getAvailableActions()\n",
    "        # select one of them randomly\n",
    "        a = np.random.choice(possibleActions)\n",
    "        # save the action\n",
    "        actions.append(a)\n",
    "        # execute the action using the step function\n",
    "        # y, x, v are new positions and velocities\n",
    "        # done is the flag whether the episode is over\n",
    "        # travel details exactly which cells the agent traveled through\n",
    "        y, x, v, r, done, travel = racer.step(action = a)\n",
    "        # save the velocities\n",
    "        velocities.append(v)\n",
    "        rewards.append(r)\n",
    "        # save the cells that the agent most recently passed through\n",
    "        # they are only needed for visualization\n",
    "        moves.append(travel)\n",
    "    \n",
    "    rewards = np.array(rewards)\n",
    "    cumulativeRewards = np.cumsum(rewards)\n",
    "    \n",
    "    #find the rewards obtained from that state onwards\n",
    "    stateRewards = np.flip(cumulativeRewards)\n",
    "    \n",
    "    #add to each state in the episode the rewards obtained from that state onwards\n",
    "    for i in range(len(stateRewards)):\n",
    "        \n",
    "        V[states[i]] += alpha*( stateRewards[i] - V[states[i]] )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        ...,\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ]],\n",
       "\n",
       "       [[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        ...,\n",
       "        [ 0.      , -0.648   , -0.52    , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      , -0.488   , -0.488   , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ]],\n",
       "\n",
       "       [[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        ...,\n",
       "        [ 0.      , -0.9808  , -0.36    , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      , -0.840256, -0.67232 , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        ...,\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ]],\n",
       "\n",
       "       [[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        ...,\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ]],\n",
       "\n",
       "       [[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        ...,\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
